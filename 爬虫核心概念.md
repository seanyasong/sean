什么是互联网爬虫
    如果我们把互联网比作一张大的蜘蛛网，那一台计算机上的数据便是蜘蛛网上的一个猎物，而爬虫程序就是一只小蜘蛛，沿着蜘蛛网抓取自己想要的数据
    通过一个程序，根据url进行爬取网页，获取有用信息
    使用程序模拟浏览器，去想服务器发送请求，获取相应信息
爬虫核心
    爬取网页，获得网页中所有的内容
    数据解析，将得来的数据进行分析
    难点：爬虫和反爬虫之间的博弈

爬虫用途
    数据分析\人工数据集
    社交软件冷启动
    舆情监控
    竞争对手监控
爬虫分类
    通用爬虫
        实例
            百度,360,google,sougou等搜索引擎
        功能
            访问网站->抓取数据->数据存储->数据处理->提供检索服务
        robots协议
            一个约定俗成的协议,添加robots.txt文件,来说明本网站那些内容不可以被抓取,起不到限制作用自己些的爬虫无需遵守
    聚焦爬虫
        1.确定要抓取的url
        2.模拟浏览器访问,获取服务器返回码
        3.解析html字符串
反爬虫手段
urllib库使用
编解码
    1.get请求方式：
    urllib.parse.quote()
    2.get请求方式：
    urllib.parse.urlencode()
    3.post请求方式
ajax的get请求
ajax的post请求
复杂get
URLError\HTTPError
cookie登陆
Handler处理器
代理服务器
cookie库

